import os
import json
import time
import logging
from typing import Any, Dict, Optional, Iterable, Tuple

import pandas as pd


class FeatureStore:
    """
    Armazena features por janela em CSV (features/features.csv por padr√£o).
    - Evita FutureWarning do pandas ao concatenar com DF vazio/todos-NA.
    - Faz flatten/serializa√ß√£o segura para campos aninhados (dict/list).
    - Garante uni√£o de colunas entre o arquivo existente e a nova linha.
    - Escrita at√¥mica + lock simples (Windows-friendly) com retry/backoff.
    - Dedup por window_id (mant√©m a √∫ltima ocorr√™ncia).
    - Rota√ß√£o opcional por tamanho/linhas.
    """

    def __init__(
        self,
        base_dir: str = "features",
        filename: str = "features.csv",
        rotate_max_bytes: Optional[int] = 20_000_000,  # ~20MB; None para desativar
        rotate_max_rows: Optional[int] = 200_000,      # None para desativar
        lock_filename: str = ".features.lock",
        backup_suffix: str = ".bak",
    ):
        self.base_dir = base_dir
        self.filename = filename
        self.file_path = os.path.join(base_dir, filename)
        self.lock_path = os.path.join(base_dir, lock_filename)
        self.backup_suffix = backup_suffix
        self.rotate_max_bytes = rotate_max_bytes
        self.rotate_max_rows = rotate_max_rows
        os.makedirs(self.base_dir, exist_ok=True)
        logging.info(f"‚úÖ FeatureStore inicializado. Dados salvos em: {self.base_dir}")

    # -------------------- utils de locking/atomicidade --------------------

    def _acquire_lock(self, timeout: float = 5.0, interval: float = 0.05) -> bool:
        """Lock por arquivo usando O_CREAT|O_EXCL via open exclusivo (cross-platform simples)."""
        deadline = time.time() + timeout
        while time.time() < deadline:
            try:
                # modo exclusivo: falha se j√° existir
                fd = os.open(self.lock_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                os.close(fd)
                return True
            except FileExistsError:
                time.sleep(interval)
            except Exception as e:
                logging.warning(f"FeatureStore: erro inesperado ao adquirir lock: {e}")
                time.sleep(interval)
        return False

    def _release_lock(self) -> None:
        try:
            if os.path.exists(self.lock_path):
                os.remove(self.lock_path)
        except Exception as e:
            logging.warning(f"FeatureStore: falha ao liberar lock: {e}")

    def _atomic_write_csv(self, df: pd.DataFrame, target: str) -> None:
        """Escreve CSV em arquivo tempor√°rio e faz os.replace (at√¥mico)."""
        tmp_path = target + ".tmp"
        # newline='' para n√£o duplicar linhas no Windows; utf-8-sig evita problemas em Excel
        df.to_csv(tmp_path, index=False)
        os.replace(tmp_path, target)

    # -------------------- flatten/serializa√ß√£o/IO --------------------

    @staticmethod
    def _flatten(d: Dict[str, Any], parent: str = "", sep: str = ".") -> Dict[str, Any]:
        out: Dict[str, Any] = {}
        for k, v in (d or {}).items():
            key = f"{parent}{sep}{k}" if parent else k
            if isinstance(v, dict):
                out.update(FeatureStore._flatten(v, key, sep))
            else:
                out[key] = v
        return out

    @staticmethod
    def _to_serializable(v: Any) -> Any:
        # Serializa estruturas complexas; normaliza NaN/Inf
        try:
            if isinstance(v, (dict, list, tuple)):
                return json.dumps(v, ensure_ascii=False)
            if isinstance(v, float):
                if v != v or v in (float("inf"), float("-inf")):
                    return None
            return v
        except Exception:
            return str(v)

    def _read_existing(self) -> Optional[pd.DataFrame]:
        if not os.path.exists(self.file_path):
            return None
        try:
            # dtype='string' para colunas texto; deixa num√©ricos serem inferidos
            df = pd.read_csv(self.file_path)
            if df is None or df.empty:
                return None
            return df
        except Exception as e:
            logging.warning(f"FeatureStore: falha ao ler arquivo existente ({self.file_path}): {e}")
            return None

    # -------------------- rota√ß√£o --------------------

    def _should_rotate(self, df_new_rows: int = 1) -> Tuple[bool, str]:
        """
        Decide por rota√ß√£o baseada em bytes ou n√∫mero de linhas.
        Retorna (bool_should_rotate, new_path_for_rotation).
        """
        if not os.path.exists(self.file_path):
            return (False, "")

        try:
            by_size = False
            if self.rotate_max_bytes is not None:
                size = os.path.getsize(self.file_path)
                by_size = size >= self.rotate_max_bytes

            by_rows = False
            if self.rotate_max_rows is not None:
                # Leitura s√≥ do header + tail r√°pido seria ideal; como √© CSV simples,
                # optamos por uma leitura leve de shape quando necess√°rio.
                # Custo √© amortizado se rotate estiver desativado.
                df = self._read_existing()
                if df is not None and not df.empty:
                    by_rows = (len(df) + df_new_rows) > self.rotate_max_rows

            if by_size or by_rows:
                # gera nome com timestamp para arquivo antigo
                ts = time.strftime("%Y%m%d-%H%M%S")
                new_path = self.file_path.replace(".csv", f".{ts}.csv")
                return (True, new_path)
        except Exception as e:
            logging.warning(f"FeatureStore: erro ao avaliar rota√ß√£o: {e}")

        return (False, "")

    def _rotate_now(self, rotated_path: str) -> None:
        try:
            # faz um backup simples e limpa arquivo atual
            os.replace(self.file_path, rotated_path)
            logging.info(f"üåÄ FeatureStore: arquivo rotacionado para {rotated_path}")
        except FileNotFoundError:
            pass
        except Exception as e:
            logging.warning(f"FeatureStore: falha na rota√ß√£o ({e}). Prosseguindo sem rotacionar.")

    # -------------------- API p√∫blica --------------------

    def save_features(self, window_id: str, features: Dict[str, Any]) -> None:
        """
        Salva uma linha de features referente √† window_id.
        - N√£o concatena DataFrames vazios/todos-NA.
        - Une colunas com o arquivo existente para evitar dtype/colunas inconsistentes.
        - Escrita at√¥mica + lock + retry/backoff.
        - Dedup por window_id.
        """
        if not isinstance(features, dict):
            logging.warning("FeatureStore: features n√£o √© dict; ignorado.")
            return

        # Flatten + serializa√ß√£o segura
        flat = self._flatten(features)
        flat = {k: self._to_serializable(v) for k, v in flat.items()}
        flat["window_id"] = window_id

        row_df = pd.DataFrame([flat])

        # Se a linha nova √© toda NA (ap√≥s serializa√ß√£o), n√£o escreve
        if row_df.drop(columns=["window_id"], errors="ignore").dropna(how="all").empty:
            logging.debug("FeatureStore: linha de features toda NA; n√£o ser√° gravada.")
            return

        # Retry curto em caso de sharing violation no Windows
        for attempt in range(6):  # ~ 0.8s total (0.05 * (2**5))
            got_lock = self._acquire_lock(timeout=0.5)
            if not got_lock:
                # n√£o conseguiu lock; backoff e tenta de novo
                time.sleep(0.05 * (2 ** attempt))
                continue

            try:
                existing_df = self._read_existing()

                if existing_df is None or existing_df.empty:
                    # arquivo n√£o existe ou est√° vazio ‚Üí escreve do zero (at√¥mico)
                    # Garantimos que n√£o existem colunas todos-NA:
                    cleaned = row_df.dropna(axis=1, how="all")
                    self._atomic_write_csv(cleaned, self.file_path)
                    return

                # Unifica colunas entre existente e nova linha, preservando ordem
                union_cols = list(dict.fromkeys(list(existing_df.columns) + list(row_df.columns)))
                existing_df = existing_df.reindex(columns=union_cols)
                row_df = row_df.reindex(columns=union_cols)

                # Concat final
                final_df = pd.concat([existing_df, row_df], ignore_index=True, sort=False, copy=False)
                # Dedup por window_id (mant√©m a √∫ltima ocorr√™ncia)
                if "window_id" in final_df.columns:
                    final_df = final_df.drop_duplicates(subset=["window_id"], keep="last", ignore_index=True)

                # Opcional: dropar colunas 100% NA (mant√©m as relevantes)
                # (ativa por padr√£o para blindar contra FutureWarning de all-NA)
                final_df = final_df.dropna(axis=1, how="all")

                # Rota√ß√£o se necess√°rio
                rotate, rotated_path = self._should_rotate(df_new_rows=0)
                if rotate:
                    self._rotate_now(rotated_path)

                # Escrita at√¥mica
                self._atomic_write_csv(final_df, self.file_path)
                return

            except Exception as e:
                logging.error(f"FeatureStore: erro ao salvar features (tentativa {attempt+1}): {e}")
                # pequeno backoff antes de tentar novamente
                time.sleep(0.05 * (2 ** attempt))

            finally:
                self._release_lock()

        # Se chegou aqui, todas as tentativas falharam
        logging.critical("FeatureStore: falha repetida ao salvar features ap√≥s m√∫ltiplas tentativas.")
