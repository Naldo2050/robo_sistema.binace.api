# ai_analyzer_qwen.py v2.4.0 + PATCH 2 - COM INTELIGÃŠNCIA QUANTITATIVA COMO BASE PRINCIPAL
"""
AI Analyzer para eventos de mercado com validaÃ§Ã£o de dados.

ğŸ”¹ NOVIDADES v2.3.0:
  âœ… Modo texto livre para GroqCloud (sem JSON estruturado)
     - resposta em texto natural
     - foco em anÃ¡lise qualitativa
     - compatibilidade total com modelos Groq
  âœ… Prompts ajustados para foco em:
     - suporte / resistÃªncia
     - absorÃ§Ã£o / exaustÃ£o
     - falta de demanda / oferta
     - pontos claros de entrada e zonas de defesa/invalidaÃ§Ã£o

ğŸ”¹ NOVIDADES v2.2.0:
  âœ… Templates de prompt com Jinja2 (se disponÃ­vel), com fallback para f-strings
  âœ… Cliente assÃ­ncrono AsyncOpenAI para Groq/OpenAI (usado internamente via asyncio.run)

ğŸ”¹ NOVIDADES v2.1.0:
   âœ… Suporte completo ao GroqCloud (PRIORIDADE 1)
   âœ… Fallback inteligente: Groq â†’ OpenAI â†’ Mock (DashScope desabilitado)
   âœ… ValidaÃ§Ã£o automÃ¡tica de chave Groq
   âœ… Logs detalhados de qual provedor estÃ¡ ativo

ğŸ”¹ CORREÃ‡Ã•ES v2.0.2 (mantidas):
  âœ… MÃ©todo analyze() adicionado para compatibilidade com main.py
  âœ… Corrige extraÃ§Ã£o de orderbook (pega do lugar certo)
  âœ… Valida orderbook_data ANTES de formatar
  âœ… Detecta contradiÃ§Ãµes corretamente (volumes vs ratio)
  âœ… Logs mais claros sobre fonte dos dados
  âœ… Fallback para mÃºltiplos caminhos de dados
  âœ… System prompt melhorado

ğŸ”¹ NOVIDADE v2.3.x (ESTA VERSÃƒO):
  âœ… IntegraÃ§Ã£o com HealthMonitor via heartbeat periÃ³dico:
     - AIAnalyzer pode receber um HealthMonitor externo
     - Envia heartbeat("ai") a cada 30s enquanto ativo
     - Fecha a thread de heartbeat no close()

ğŸ”¹ PATCH 2 - IMPLEMENTADO:
  âœ… Impede fallback automÃ¡tico para provider errado
  âœ… Se provider=groq, tenta modelos alternativos da Groq apenas
  âœ… Se nenhum modelo Groq funcionar, vai para MOCK (nÃ£o OpenAI automaticamente)
  âœ… SÃ³ troca para outro provider se explicitamente configurado em provider_fallbacks
  âœ… Fallback automÃ¡tico DESABILITADO por padrÃ£o
"""

import logging
import os
import random
import time
import asyncio
import threading
import json
from typing import Any, Dict, Optional, Literal, TYPE_CHECKING

from dotenv import load_dotenv

from format_utils import (
    format_price,
    format_quantity,
    format_percent,
    format_large_number,
    format_delta,
    format_time_seconds,
    format_scientific
)

try:
    import config as app_config
except Exception:
    app_config = None

# Import opcional do HealthMonitor (para type hint e uso de heartbeat)
if TYPE_CHECKING:
    # Import usado apenas para type checking (Pylance, mypy etc.)
    from health_monitor import HealthMonitor

# OpenAI / Groq (cliente sÃ­ncrono + assÃ­ncrono)
try:
    from openai import OpenAI, AsyncOpenAI
    OPENAI_AVAILABLE = True
    ASYNC_OPENAI_AVAILABLE = True
except Exception:
    try:
        from openai import OpenAI  # type: ignore
        OPENAI_AVAILABLE = True
    except Exception:
        OPENAI_AVAILABLE = False
    ASYNC_OPENAI_AVAILABLE = False
    AsyncOpenAI = None  # type: ignore

# DashScope
try:
    from dashscope import Generation
    import dashscope
    DASHSCOPE_AVAILABLE = True
except Exception:
    DASHSCOPE_AVAILABLE = False

# Jinja2 para templates (opcional)
try:
    from jinja2 import Environment, BaseLoader
    JINJA_AVAILABLE = True
except Exception:
    JINJA_AVAILABLE = False
    Environment = None  # type: ignore
    BaseLoader = object  # type: ignore

# Pydantic para structured output (opcional)
try:
    from pydantic import BaseModel
    PYDANTIC_AVAILABLE = True
except Exception:
    PYDANTIC_AVAILABLE = False
    BaseModel = object  # type: ignore

from time_manager import TimeManager
from orderbook_core.structured_logging import StructuredLogger

# Regime Detector integration
try:
    from src.analysis.regime_detector import RegimeDetector
    REGIME_DETECTOR_AVAILABLE = True
except ImportError as e:
    logging.warning(f"Regime detector nÃ£o disponÃ­vel: {e}")
    REGIME_DETECTOR_AVAILABLE = False

load_dotenv()



if PYDANTIC_AVAILABLE:
    class AITradeAnalysis(BaseModel):
        """
        Esquema estruturado para a resposta da IA.

        Focado em:
        - direÃ§Ã£o (sentiment)
        - forÃ§a (confidence)
        - aÃ§Ã£o sugerida (action)
        - regiÃ£o de entrada / zona de defesa (entry_zone)
        - zona de invalidaÃ§Ã£o (invalidation_zone)
        - tipo de regiÃ£o (region_type: suporte/resistÃªncia/absorÃ§Ã£o/exaustÃ£o/etc.)
        """
        sentiment: Literal["bullish", "bearish", "neutral"]
        confidence: float  # 0.0â€“1.0
        action: Literal["buy", "sell", "hold", "flat", "wait", "avoid"]
        rationale: str
        entry_zone: Optional[str] = None
        invalidation_zone: Optional[str] = None
        region_type: Optional[str] = None
else:
    AITradeAnalysis = None  # type: ignore


# ========================
# Jinja2 Templates
# ========================

if JINJA_AVAILABLE:
    _jinja_env = Environment(loader=BaseLoader(), trim_blocks=True, lstrip_blocks=True)
else:
    _jinja_env = None

# ========================
# SYSTEM PROMPT (v2.4.0)
# ========================

SYSTEM_PROMPT = """VocÃª Ã© analista institucional de fluxo, suporte/resistÃªncia e regiÃµes de defesa.

ğŸ”¹ HORIZONTE DE ANÃLISE: Focado em entradas rÃ¡pidas de 5-15 minutos (scalp), com validaÃ§Ã£o em horizontes maiores se disponÃ­vel.

ğŸ”¹ OBJETIVO: Identificar regiÃµes de entrada claras para trades curtos, priorizando defesa institucional (absorÃ§Ã£o) e pontos de invalidaÃ§Ã£o tÃ©cnicos. NÃ£o force trades em ruÃ­do.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ§  REGRA FUNDAMENTAL: INTELIGÃŠNCIA QUANTITATIVA Ã‰ A BASE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

O modelo XGBoost fornece probabilidades matemÃ¡ticas (prob_up, prob_down) e um action_bias (compra/venda/aguardar).

âœ… USE A INTELIGÃŠNCIA QUANTITATIVA COMO BASE PRINCIPAL:
   - O action_bias deve guiar sua decisÃ£o inicial
   - A confianÃ§a do modelo (confidence_score) indica forÃ§a da previsÃ£o

âš ï¸ SÃ“ VÃ CONTRA O VIÃ‰S MATEMÃTICO SE:
   - Houver absorÃ§Ã£o MASSIVA no orderbook contrÃ¡ria ao viÃ©s
   - Whale activity significativa na direÃ§Ã£o oposta
   - CVD/Net Flow em forte divergÃªncia
   - EvidÃªncia EXTREMA de exaustÃ£o/reversÃ£o no fluxo

Se nÃ£o houver evidÃªncia MUITO FORTE, SIGA o action_bias do modelo.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REGRAS GERAIS:
1) Use SOMENTE dados fornecidos explicitamente.
2) Se marcado 'IndisponÃ­vel' ou 'âš ï¸', NÃƒO use.
3) Orderbook zerado? Use fluxo (net_flow, flow_imbalance, tick_rule).
4) ContradiÃ§Ãµes? Ignore dado contraditÃ³rio.
5) Foque em identificar REGIÃ•ES IMPORTANTES:
   - Suportes e resistÃªncias relevantes (use VP: POC/VAH/VAL)
   - RegiÃµes de absorÃ§Ã£o (defesa) e exaustÃ£o (fraqueza) via fluxo/whales
   - Falta de demanda/oferta (breaks, buracos de liquidez)
6) SÃ³ sugira ENTRADA quando houver regiÃ£o CLARA e bem defendida,
   descrevendo preÃ§o aproximado e zona de invalidaÃ§Ã£o.
7) Em contexto de RUÃDO (range estreito, fluxo misto, confianÃ§a baixa):
   - Reduza sinais; prefira action='wait' ou 'avoid'.
8) Incentive action='wait' se confianÃ§a do modelo < 50% ou edge nÃ£o claro.
9) Se o cenÃ¡rio nÃ£o estiver claro, prefira recomendar 'aguardar'.
10) Seja sucinto, objetivo e profissional.

11) Responda SEMPRE e APENAS em portuguÃªs do Brasil.
12) NÃƒO utilize inglÃªs em nenhuma parte da resposta.
13) NÃƒO use tags <think> nem mostre seu raciocÃ­nio passo a passo; entregue apenas a anÃ¡lise final em portuguÃªs.
14) GARANTA que toda sua comunicaÃ§Ã£o seja em portuguÃªs brasileiro, sem mistura de idiomas.

Responda sempre e apenas em portuguÃªs do Brasil.
NÃ£o utilize inglÃªs em nenhuma parte da resposta.
NÃ£o use tags <think> nem mostre seu raciocÃ­nio passo a passo; entregue apenas a anÃ¡lise final em portuguÃªs.
"""



ORDERBOOK_TEMPLATE = """
ğŸ§  **AnÃ¡lise Institucional â€“ {{ ativo }} | {{ tipo_evento }}**

ğŸ“ DescriÃ§Ã£o: {{ descricao }}
{{ ob_str }}{{ ml_str }}{{ vp_str }}{{ order_flow_str }}

ğŸ“ˆ Multi-Timeframes
{{ multi_tf_str }}

â³ MemÃ³ria de eventos
{{ memoria_str }}

ğŸ“‰ Probabilidade HistÃ³rica
   Long={{ prob_long }} | Short={{ prob_short }} | Neutro={{ prob_neutral }}

ğŸ¯ Tarefa
CRÃTICO: Se dados estiverem marcados como "IndisponÃ­vel" ou "âš ï¸", NÃƒO os use.

{% if not is_orderbook_valid %}
ğŸ”´ ORDERBOOK INDISPONÃVEL - Use APENAS mÃ©tricas de fluxo (net_flow, flow_imbalance, tick_rule)
{% endif %}

Foque em:
1) Identificar regiÃµes importantes:
   - suportes/resistÃªncias relevantes
   - Ã¡reas de absorÃ§Ã£o (defesa) e exaustÃ£o (fraqueza)
   - buracos de liquidez (falta de demanda/oferta)
2) Sugerir, SE HOUVER clareza, uma regiÃ£o aproximada de entrada (entry_zone) e uma zona de invalidaÃ§Ã£o (invalidation_zone).
3) Se o cenÃ¡rio nÃ£o estiver claro, recomende aguardar (sem forÃ§ar trade).

Se dados crÃ­ticos faltarem, seja explÃ­cito sobre limitaÃ§Ãµes.
"""

DEFAULT_TEMPLATE = """
ğŸ§  **AnÃ¡lise Institucional â€“ {{ ativo }} | {{ tipo_evento }}**

ğŸ“ DescriÃ§Ã£o: {{ descricao }}

   PreÃ§o: {{ preco_fmt }}
   Delta: {{ delta_line }}
   Volume: {{ vol_line }}
{{ ml_str }}{{ vp_str }}{{ order_flow_str }}

ğŸ“ˆ Multi-Timeframes
{{ multi_tf_str }}

â³ MemÃ³ria de eventos
{{ memoria_str }}

ğŸ“‰ Probabilidade HistÃ³rica
   Long={{ prob_long }} | Short={{ prob_short }} | Neutro={{ prob_neutral }}

ğŸ¯ Tarefa
Use APENAS dados explicitamente fornecidos.
Se marcado como "IndisponÃ­vel", NÃƒO use na anÃ¡lise.

Foque em:
1) ForÃ§a ou fraqueza do movimento (sentimento).
2) PresenÃ§a de regiÃ£o de defesa (suporte/absorÃ§Ã£o) ou oferta (resistÃªncia/exaustÃ£o).
3) Se houver cenÃ¡rios claros, descreva:
   - regiÃ£o aproximada de entrada (entry_zone)
   - zona de invalidaÃ§Ã£o (invalidation_zone)
4) Se nÃ£o houver entrada clara, recomende aguardar (wait/avoid) e explique o porquÃª.
"""


def _is_model_decommissioned_error(err: Exception) -> bool:
    """
    Detecta o erro retornado pela API (via SDK OpenAI compatÃ­vel) quando o modelo foi descontinuado.
    Fazemos por string + tentativa de ler 'body' quando existir.
    """
    msg = str(err)
    if "model_decommissioned" in msg:
        return True

    body = getattr(err, "body", None)
    if isinstance(body, dict):
        code = (body.get("error") or {}).get("code")
        if code == "model_decommissioned":
            return True

    return False


def _extract_dashscope_text(resp) -> str:
    """Extrai texto de respostas do DashScope."""
    try:
        output = getattr(resp, "output", None)
        if output is None and isinstance(resp, dict):
            output = resp.get("output")
        if not output:
            return ""
        choices = getattr(output, "choices", None)
        if choices is None and isinstance(output, dict):
            choices = output.get("choices")
        if not choices:
            return ""
        choice0 = choices[0]
        message = getattr(choice0, "message", None)
        if message is None and isinstance(choice0, dict):
            message = choice0.get("message")
        content = getattr(message, "content", None)
        if content is None and isinstance(message, dict):
            content = message.get("content")
        if isinstance(content, list) and content:
            for part in content:
                if isinstance(part, dict) and part.get("text"):
                    return str(part["text"]).strip()
        if isinstance(message, str):
            return message.strip()
        return ""
    except Exception:
        return ""


def _dedupe_keep_order(items):
    out = []
    for x in items:
        if x and x not in out:
            out.append(x)
    return out


def _models_from_cfg(cfg: dict) -> list[str]:
    primary = cfg.get("model", "qwen/qwen3-32b")
    fallbacks = cfg.get("model_fallbacks", [])
    if not isinstance(fallbacks, list):
        fallbacks = []
    return _dedupe_keep_order([primary, *fallbacks])


class AIAnalyzer:
    """Analisador de IA com validaÃ§Ã£o robusta de dados e suporte GroqCloud + structured output + heartbeat."""
    
    def __init__(
        self,
        health_monitor: Optional["HealthMonitor"] = None,
        module_name: str = "ai",
    ):
        """
        health_monitor: instÃ¢ncia de HealthMonitor (opcional).
        module_name: nome usado para registrar heartbeat (default: 'ai').

        CompatÃ­vel com chamadas antigas: AIAnalyzer() continua funcionando.
        """
        self.client: Optional[Any] = None        # cliente sÃ­ncrono
        self.client_async: Optional[Any] = None  # cliente assÃ­ncrono (AsyncOpenAI)
        self.enabled = False
        self.mode: Optional[str] = None
        self.time_manager = TimeManager()

        # Logger estruturado interno da IA
        self.slog = StructuredLogger("ai_analyzer", "AI")

        # IntegraÃ§Ã£o com HealthMonitor
        self.health_monitor = health_monitor
        self.module_name = module_name
        self._hb_stop = threading.Event()
        self._hb_thread: Optional[threading.Thread] = None

        # Carrega configuraÃ§Ã£o do config.json
        self.config = {}
        try:
            with open('config.json', 'r') as f:
                self.config = json.load(f)
        except Exception as e:
            logging.warning(f"Erro ao carregar config.json: {e}")

        # Modelo padrÃ£o (serÃ¡ sobrescrito se Groq estiver ativo)
        self.model_name = (
            getattr(app_config, "QWEN_MODEL", None)
            or os.getenv("QWEN_MODEL")
            or "qwen-plus"
        )

        self.last_test_time = 0
        self.test_interval_seconds = 120
        self.connection_failed_count = 0
        self.max_failures_before_mock = 3

        logging.info(
            "ğŸ§  IA Analyzer v2.4.0 + PATCH 2 inicializada - GroqCloud (modo texto livre)"
        )
        try:
            self._initialize_api()
        except Exception as e:
            logging.warning(f"Falha ao inicializar provedores de IA: {e}. Usando mock.")
            self.mode = None
            self.enabled = True

        # Se existir HealthMonitor, envia um primeiro heartbeat e inicia thread periÃ³dica
        if self.health_monitor is not None:
            try:
                self.health_monitor.heartbeat(self.module_name)
            except Exception:
                pass

            self._hb_thread = threading.Thread(
                target=self._heartbeat_loop,
                name=f"AIAnalyzerHeartbeat-{self.module_name}",
                daemon=True,
            )
            self._hb_thread.start()
            logging.info("ğŸ«€ Heartbeat do mÃ³dulo '%s' iniciado (AIAnalyzer).", self.module_name)

    # ---------------- HEARTBEAT LOOP ----------------
    def _heartbeat_loop(self):
        """
        Envia heartbeat periÃ³dico para o HealthMonitor enquanto o AIAnalyzer estiver ativo.
        Se o processo travar de verdade, esta thread tambÃ©m pÃ¡ra, e o HealthMonitor detecta.
        """
        # Intervalo padrÃ£o de heartbeat: 30s (pode ser ajustado lendo do config.py)
        interval = getattr(app_config, "HEALTH_CHECK_INTERVAL", 30)
        interval = max(5, min(interval, 60))  # mantÃ©m entre 5s e 60s para seguranÃ§a

        while not self._hb_stop.is_set():
            try:
                if self.health_monitor is not None:
                    self.health_monitor.heartbeat(self.module_name)
            except Exception:
                # Nunca deixe exceÃ§Ã£o matar a thread de heartbeat
                pass
            # Espera atÃ© o prÃ³ximo ciclo ou parada
            for _ in range(int(interval * 10)):  # subdivide em passos de 0.1s para reaÃ§Ã£o rÃ¡pida ao stop
                if self._hb_stop.is_set():
                    break
                time.sleep(0.1)

    # ------------------------------------------------

    def _initialize_api(self):
        """
        Inicializa provedores de IA com PATCH 2 - Fallback Controlado.
        
        âœ… NOVA LÃ“GICA (Patch 2):
          - Se provider=groq, tenta apenas modelos alternativos da Groq
          - Se nenhum modelo Groq funcionar, vai para MOCK (nÃ£o OpenAI automaticamente)
          - SÃ³ troca para outro provider se explicitamente configurado em provider_fallbacks
          - Fallback automÃ¡tico DESABILITADO por padrÃ£o
        """
        
        # Carrega configuraÃ§Ã£o do config.json
        ai_cfg = self.config.get("ai", {})
        provider = ai_cfg.get("provider", "groq")
        provider_fallbacks = ai_cfg.get("provider_fallbacks", [])
        
        # Lista para tracking de providers testados
        providers_tested = []
        
        # ============================================================
        # ğŸš€ PROVIDER ESPECÃFICO: GROQ
        # ============================================================
        if provider == "groq":
            providers_tested.append("groq")
            groq_key = os.getenv("GROQ_API_KEY") or getattr(app_config, "GROQ_API_KEY", None)
            
            if OPENAI_AVAILABLE and groq_key:
                groq_cfg = ai_cfg.get("groq", {})
                groq_base_url = groq_cfg.get("base_url", "https://api.groq.com/openai/v1")
                
                # Valida formato da chave
                if not groq_key.startswith("gsk_"):
                    logging.warning(
                        f"âš ï¸ GROQ_API_KEY suspeita (nÃ£o comeÃ§a com 'gsk_'). "
                        f"Tentando mesmo assim..."
                    )
                
                self.base_url = groq_base_url
                # Cliente sÃ­ncrono OpenAI-compatÃ­vel apontando para Groq
                self.client = OpenAI(
                    api_key=groq_key,
                    base_url=self.base_url
                )
                # Cliente assÃ­ncrono, se disponÃ­vel
                if ASYNC_OPENAI_AVAILABLE and AsyncOpenAI is not None:
                    self.client_async = AsyncOpenAI(
                        api_key=groq_key,
                        base_url=self.base_url
                    )
                logging.info("ğŸ”§ Groq client configurado | base_url=%s", self.base_url)

                models = _models_from_cfg(groq_cfg)
                self._groq_model_candidates = models
                if not models:
                    logging.warning("Nenhum modelo Groq configurado em ai.groq.model")
                else:
                    last_err = None
                    selected = None
                    for m in models:
                        try:
                            self.client.chat.completions.create(
                                model=m,
                                messages=[{"role": "user", "content": "ping"}],
                                temperature=0,
                                max_tokens=1,
                                timeout=10,
                            )
                            selected = m
                            break
                        except Exception as e:
                            last_err = e
                            logging.warning(f"âš ï¸ Ping falhou no modelo Groq {m}: {e}")
                    if not selected:
                        logging.error(f"âŒ Groq sem modelo vÃ¡lido. Ãšltimo erro: {last_err}")
                        # PATCH 2: Se Groq falhar, NÃƒO fazer fallback automÃ¡tico
                        # Vai para mock ou tentar fallbacks explicitamente configurados
                    else:
                        self.model_name = selected
                        self.mode = "groq"
                        self.enabled = True
                        logging.info(
                            f"ğŸš€ GroqCloud ATIVO | Modelo final: {self.model_name} | "
                            f"Chave: {groq_key[:10]}...{groq_key[-4:]}"
                        )
                        try:
                            self.slog.info(
                                "ai_provider_selected",
                                provider="groq",
                                model=self.model_name,
                            )
                        except Exception:
                            pass
                        return
            
            # Se chegou aqui, Groq falhou
            # PATCH 2: SÃ³ tenta fallbacks se explicitamente configurados
            if not provider_fallbacks:
                logging.info("ğŸ”§ Groq falhou e nenhum fallback configurado. Ativando modo MOCK.")
                self._activate_mock_mode()
                return
            else:
                logging.info(f"ğŸ”„ Groq falhou, tentando fallbacks configurados: {provider_fallbacks}")
        
        # ============================================================
        # FALLBACKS EXPLICITAMENTE CONFIGURADOS
        # ============================================================
        # SÃ³ chega aqui se:
        # 1. Provider nÃ£o Ã© groq, OU
        # 2. Groq falhou E tem fallbacks configurados
        
        for fallback_provider in provider_fallbacks:
            if fallback_provider in providers_tested:
                continue  # JÃ¡ testou este provider
                
            if fallback_provider == "openai":
                providers_tested.append("openai")
                if self._try_initialize_openai():
                    return
                    
            elif fallback_provider == "dashscope":
                providers_tested.append("dashscope")
                if self._try_initialize_dashscope():
                    return
        
        # ============================================================
        # PROVIDER PADRÃƒO: OPENAI (sÃ³ se nÃ£o for groq ou groq com fallback)
        # ============================================================
        if provider != "groq" and "openai" not in providers_tested:
            if self._try_initialize_openai():
                return
        
        # ============================================================
        # FALLBACK FINAL: MOCK
        # ============================================================
        self._activate_mock_mode()
        
    def _try_initialize_openai(self) -> bool:
        """Tenta inicializar OpenAI. Retorna True se succeeded."""
        try:
            self.client = OpenAI()  # Usa OPENAI_API_KEY
            if ASYNC_OPENAI_AVAILABLE and AsyncOpenAI is not None:
                self.client_async = AsyncOpenAI()
            self.mode = "openai"
            self.enabled = True
            logging.info("ğŸ”§ OpenAI client configurado (fallback)")
            try:
                self.slog.info(
                    "ai_provider_selected",
                    provider="openai",
                    model=self.model_name,
                )
            except Exception:
                pass
            return True
        except Exception as e:
            logging.warning(f"OpenAI indisponÃ­vel: {e}")
            return False
    
    def _try_initialize_dashscope(self) -> bool:
        """Tenta inicializar DashScope. Retorna True se succeeded."""
        token = os.getenv("DASHSCOPE_API_KEY") or getattr(app_config, "DASHSCOPE_API_KEY", None)
        
        if DASHSCOPE_AVAILABLE and token:
            try:
                import dashscope
                dashscope.api_key = token
                self.mode = "dashscope"
                self.enabled = True
                logging.info("ğŸ”§ DashScope configurado (fallback)")
                try:
                    self.slog.info(
                        "ai_provider_selected",
                        provider="dashscope",
                        model=self.model_name,
                    )
                except Exception:
                    pass
                return True
            except Exception as e:
                logging.warning(f"DashScope indisponÃ­vel: {e}")
        
        return False
    
    def _activate_mock_mode(self):
        """Ativa modo mock."""
        self.mode = None
        self.enabled = True
        logging.info("ğŸ”§ Modo MOCK ativado (sem provedores externos).")
        try:
            self.slog.warning(
                "ai_provider_selected",
                provider="mock",
                model=self.model_name,
            )
        except Exception:
            pass

    async def _select_working_groq_model(self) -> str | None:
        """Seleciona um modelo Groq vÃ¡lido testando cada candidato."""
        if not self._groq_model_candidates:
            return None

        last_err = None
        for candidate in self._groq_model_candidates:
            try:
                # Temporariamente define o modelo para teste
                original_model = self.model_name
                self.model_name = candidate

                # Testa conexÃ£o (ping)
                if await self._ping_once_async():
                    # Sucesso: mantÃ©m o modelo selecionado
                    logging.info(f"âœ… Modelo Groq vÃ¡lido encontrado: {candidate}")
                    return candidate
                else:
                    logging.warning(f"âš ï¸ Ping falhou para modelo {candidate}")

            except Exception as e:
                last_err = e
                if _is_model_decommissioned_error(e):
                    logging.warning(f"âš ï¸ Modelo Groq descontinuado: {candidate} | tentando prÃ³ximo fallback...")
                    continue
                else:
                    logging.warning(f"âš ï¸ Falha ao testar modelo Groq {candidate}: {e}")
                    continue
            finally:
                # Restaura modelo original se falhou
                if 'original_model' in locals():
                    self.model_name = original_model

        logging.error(f"âŒ Nenhum modelo Groq funcionou. Ãšltimo erro: {last_err}")
        return None

    async def _ping_once_async(self) -> bool:
        """VersÃ£o assÃ­ncrona do ping para um modelo."""
        try:
            # Tenta uma chamada simples
            response = await self.client_async.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "DiagnÃ³stico curto. Responda apenas 'OK'."},
                    {"role": "user", "content": "Ping curto. Responda com 'OK'."},
                ],
                max_tokens=3,
                temperature=0.0,
                timeout=10,
            )
            content = response.choices[0].message.content.strip().upper()
            return content.startswith("OK")
        except Exception:
            return False

    def _should_test_connection(self) -> bool:
        """Verifica se deve testar conexÃ£o."""
        now = time.time()
        return (now - self.last_test_time) >= self.test_interval_seconds

    def _test_connection(self) -> bool:
        """Testa conexÃ£o com IA (ping curto)."""
        if self.mode is None and not self.client:
            try:
                self._initialize_api()
            except Exception:
                pass

        prompt = "Ping curto. Responda com 'OK'."
        ok = True
        try:
            if self.mode == "openai" or self.mode == "groq":
                r = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "DiagnÃ³stico curto. Responda apenas 'OK'."},
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=3,
                    temperature=0.0,
                    timeout=10,
                )
                content = r.choices[0].message.content.strip().upper()
                success = content.startswith("OK")
                
                if success and self.mode == "groq":
                    logging.debug("âœ… Groq ping OK")
                
                ok = success
                
            elif self.mode == "dashscope":
                r = Generation.call(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "DiagnÃ³stico curto. Responda apenas 'OK'."},
                        {"role": "user", "content": prompt},
                    ],
                    result_format="message",
                    max_tokens=3,
                    temperature=0.0,
                    timeout=10,
                )
                content = _extract_dashscope_text(r).upper()
                ok = content.startswith("OK")
            else:
                ok = True  # Mock sempre OK
                
        except Exception as e:
            self.connection_failed_count += 1
            logging.warning(
                f"Falha no ping da IA [{self.mode}] ({self.connection_failed_count}): {e}"
            )
            ok = False

        # log estruturado
        try:
            if ok:
                self.slog.info(
                    "ai_ping_ok",
                    mode=self.mode or "mock",
                )
            else:
                self.slog.warning(
                    "ai_ping_failed",
                    mode=self.mode or "mock",
                    failures=self.connection_failed_count,
                )
        except Exception:
            pass

        return ok

    # ====================================================================
    # ğŸ†• EXTRAÃ‡ÃƒO DE DADOS CORRIGIDA (mantido de v2.0.2)
    # ====================================================================
    
    def _extract_orderbook_data(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extrai dados de orderbook de mÃºltiplas fontes possÃ­veis.
        """
        candidates = [
            event_data.get('orderbook_data'),
            event_data.get('spread_metrics'),
            (event_data.get('contextual_snapshot') or {}).get('orderbook_data'),
            (event_data.get('contextual') or {}).get('orderbook_data'),
        ]
        
        for i, candidate in enumerate(candidates, 1):
            if not isinstance(candidate, dict):
                continue
            
            has_depth = (
                candidate.get('bid_depth_usd') is not None or
                candidate.get('ask_depth_usd') is not None
            )
            
            if has_depth:
                bid_usd = float(candidate.get('bid_depth_usd', 0) or 0)
                ask_usd = float(candidate.get('ask_depth_usd', 0) or 0)
                
                if bid_usd > 0 and ask_usd > 0:
                    logging.debug(f"âœ… Orderbook extraÃ­do da fonte #{i}: bid=${bid_usd:,.0f}, ask=${ask_usd:,.0f}")
                    return candidate
                else:
                    logging.debug(f"âš ï¸ Fonte #{i} tem dados zerados (bid=${bid_usd}, ask=${ask_usd})")
        
        logging.warning("âš ï¸ Nenhuma fonte de orderbook vÃ¡lida encontrada")
        return {}

    # ====================================================================
    # PROMPT BUILDER COM VALIDAÃ‡ÃƒO ROBUSTA (usando Jinja2 se disponÃ­vel)
    # ====================================================================
    
    def _render_template(self, template_name: str, context: Dict[str, Any]) -> str:
        """Renderiza template com Jinja2 se disponÃ­vel, senÃ£o usa concatenaÃ§Ã£o simples."""
        if template_name == "orderbook":
            tmpl_str = ORDERBOOK_TEMPLATE
        else:
            tmpl_str = DEFAULT_TEMPLATE

        if JINJA_AVAILABLE and _jinja_env is not None:
            try:
                tmpl = _jinja_env.from_string(tmpl_str)
                return tmpl.render(**context)
            except Exception as e:
                logging.error(f"Erro ao renderizar template Jinja2: {e}")

        # Fallback simples sem Jinja2
        if template_name == "orderbook":
            return (
                f"ğŸ§  **AnÃ¡lise Institucional â€“ {context['ativo']} | {context['tipo_evento']}**\n\n"
                f"ğŸ“ DescriÃ§Ã£o: {context['descricao']}\n"
                f"{context['ob_str']}{context['ml_str']}{context['vp_str']}{context['order_flow_str']}\n\n"
                f"ğŸ“ˆ Multi-Timeframes\n{context['multi_tf_str']}\n\n"
                f"â³ MemÃ³ria de eventos\n{context['memoria_str']}\n\n"
                f"ğŸ“‰ Probabilidade HistÃ³rica\n"
                f"   Long={context['prob_long']} | Short={context['prob_short']} | Neutro={context['prob_neutral']}\n\n"
                "ğŸ¯ Tarefa\n"
                "CRÃTICO: Se dados estiverem marcados como \"IndisponÃ­vel\" ou \"âš ï¸\", NÃƒO os use.\n\n"
                + ("ğŸ”´ ORDERBOOK INDISPONÃVEL - Use APENAS mÃ©tricas de fluxo (net_flow, flow_imbalance, tick_rule)\n\n"
                   if not context.get("is_orderbook_valid", True) else "")
                + "Foque em identificar regiÃµes importantes (suporte/resistÃªncia, absorÃ§Ã£o, exaustÃ£o, buracos de liquidez)\n"
                "e em sugerir, SE HOUVER clareza, uma regiÃ£o de entrada e zona de invalidaÃ§Ã£o.\n"
                "Se dados crÃ­ticos faltarem, seja explÃ­cito sobre limitaÃ§Ãµes.\n"
            )
        else:
            return (
                f"ğŸ§  **AnÃ¡lise Institucional â€“ {context['ativo']} | {context['tipo_evento']}**\n\n"
                f"ğŸ“ DescriÃ§Ã£o: {context['descricao']}\n\n"
                f"   PreÃ§o: {context['preco_fmt']}\n"
                f"   Delta: {context['delta_line']}\n"
                f"   Volume: {context['vol_line']}\n"
                f"{context['ml_str']}{context['vp_str']}{context['order_flow_str']}\n\n"
                "ğŸ“ˆ Multi-Timeframes\n"
                f"{context['multi_tf_str']}\n\n"
                "â³ MemÃ³ria de eventos\n"
                f"{context['memoria_str']}\n\n"
                "ğŸ“‰ Probabilidade HistÃ³rica\n"
                f"   Long={context['prob_long']} | Short={context['prob_short']} | Neutro={context['prob_neutral']}\n\n"
                "ğŸ¯ Tarefa\n"
                "Use APENAS dados explicitamente fornecidos.\n"
                "Se marcado como \"IndisponÃ­vel\", NÃƒO use na anÃ¡lise.\n\n"
                "Foque em:\n"
                "1) ForÃ§a ou fraqueza do movimento (sentimento).\n"
                "2) PresenÃ§a de regiÃ£o de defesa (suporte/absorÃ§Ã£o) ou oferta (resistÃªncia/exaustÃ£o).\n"
                "3) Se houver cenÃ¡rios claros, descreva:\n"
                "   - regiÃ£o aproximada de entrada (entry_zone)\n"
                "   - zona de invalidaÃ§Ã£o (invalidation_zone)\n"
                "4) Se nÃ£o houver entrada clara, recomende aguardar (wait/avoid) e explique o porquÃª.\n"
            )



    def _create_prompt(self, event_data: Dict[str, Any]) -> str:
        """
        Cria prompt para IA.
        [MODIFICADO] Verifica se existe 'ai_payload' estruturado antes de usar lÃ³gica legada.
        """
        ai_payload = event_data.get("ai_payload")
        if ai_payload and isinstance(ai_payload, dict):
            try:
                logging.debug("Usando ai_payload estruturado para montar o prompt")
                return self._build_structured_prompt(ai_payload)
            except Exception as e:
                logging.error(
                    f"Erro ao construir prompt estruturado: {e}. "
                    f"Usando fallback legado.",
                    exc_info=True,
                )
        tipo_evento = event_data.get("tipo_evento", "N/A")
        ativo = event_data.get("ativo") or event_data.get("symbol") or "N/A"
        descricao = event_data.get("descricao", "Sem descriÃ§Ã£o.")
        
        # Orderbook
        ob_data = self._extract_orderbook_data(event_data)
        bid_usd_raw = float(ob_data.get('bid_depth_usd', 0) or 0)
        ask_usd_raw = float(ob_data.get('ask_depth_usd', 0) or 0)
        is_orderbook_valid = (bid_usd_raw > 0 and ask_usd_raw > 0)
        
        if not is_orderbook_valid:
            logging.warning(
                f"âš ï¸ Orderbook INVÃLIDO para prompt: bid=${bid_usd_raw}, ask=${ask_usd_raw}"
            )
        
        # Delta e volumes
        delta_raw = event_data.get("delta")
        volume_total_raw = event_data.get("volume_total")
        volume_compra_raw = event_data.get("volume_compra")
        volume_venda_raw = event_data.get("volume_venda")
        
        delta = float(delta_raw) if delta_raw is not None else None
        volume_total = float(volume_total_raw) if volume_total_raw is not None else None
        volume_compra = float(volume_compra_raw) if volume_compra_raw is not None else None
        volume_venda = float(volume_venda_raw) if volume_venda_raw is not None else None
        
        # ConsistÃªncia delta vs volumes
        if delta is not None and abs(delta) > 1.0:
            if (volume_compra == 0 and volume_venda == 0) or volume_total == 0:
                logging.warning(
                    f"âš ï¸ InconsistÃªncia: delta={delta:.2f} mas volumes zerados. "
                    f"Marcando volumes como indisponÃ­veis."
                )
                volume_compra = None
                volume_venda = None
                volume_total = None
        
        preco = (
            event_data.get("preco_atual")
            or event_data.get("preco_fechamento")
            or (event_data.get("ohlc", {}) or {}).get("close")
            or 0
        )

        # Multi TF
        multi_tf = (
            event_data.get("multi_tf")
            or event_data.get("contextual_snapshot", {}).get("multi_tf")
            or event_data.get("contextual", {}).get("multi_tf")
            or {}
        )
        multi_tf_str = (
            "\n".join(f"- {tf}: {v}" for tf, v in multi_tf.items()) if multi_tf else "IndisponÃ­vel."
        )

        # MemÃ³ria de eventos
        memoria = event_data.get("event_history", [])
        if memoria:
            mem_lines = []
            for e in memoria:
                mem_delta = format_delta(e.get('delta', 0))
                mem_vol = format_large_number(e.get('volume_total', 0))
                mem_lines.append(
                    f"   - {e.get('timestamp')} | {e.get('tipo_evento')} "
                    f"{e.get('resultado_da_batalha')} (Î”={mem_delta}, Vol={mem_vol})"
                )
            memoria_str = "\n".join(mem_lines)
        else:
            memoria_str = "   Nenhum evento recente."

        # Probabilidade histÃ³rica
        conf = event_data.get("historical_confidence", {})
        prob_long = conf.get("long_prob", "IndisponÃ­vel")
        prob_short = conf.get("short_prob", "IndisponÃ­vel")
        prob_neutral = conf.get("neutral_prob", "IndisponÃ­vel")

        # Volume Profile
        vp = (
            event_data.get("historical_vp", {}).get("daily", {})
            or event_data.get("contextual_snapshot", {}).get("historical_vp", {}).get("daily", {})
            or {}
        )
        vp_str = ""
        if vp:
            poc_fmt = format_price(vp.get('poc', 0))
            val_fmt = format_price(vp.get('val', 0))
            vah_fmt = format_price(vp.get('vah', 0))
            vp_str = f"""
ğŸ“Š Volume Profile (DiÃ¡rio)
   POC: ${poc_fmt} | VAL: ${val_fmt} | VAH: ${vah_fmt}
"""

        # Order flow
        flow = (
            event_data.get("fluxo_continuo")
            or event_data.get("flow_metrics")
            or (event_data.get("contextual_snapshot") or {}).get("flow_metrics")
            or (event_data.get("contextual") or {}).get("flow_metrics")
            or {}
        )
        
        order_flow_str = ""
        if isinstance(flow, dict) and flow:
            of = flow.get("order_flow", {})
            if isinstance(of, dict) and of:
                try:
                    buy_vol = of.get("buy_volume", 0)
                    sell_vol = of.get("sell_volume", 0)
                    bsr = of.get("buy_sell_ratio")
                    
                    has_volumes = (buy_vol > 0 or sell_vol > 0)
                    
                    if not has_volumes and bsr is not None and bsr > 0:
                        logging.warning(
                            f"âš ï¸ CONTRADIÃ‡ÃƒO: buy/sell volumes zero mas ratio={bsr}. "
                            f"Marcando ratio como indisponÃ­vel."
                        )
                        bsr = None
                    
                    flow_lines = []
                    
                    # Net flows
                    nf1 = of.get("net_flow_1m")
                    if nf1 is not None:
                        flow_lines.append(f"   Net Flow 1m: {format_delta(nf1)}")
                    
                    # Flow imbalance
                    fi = of.get("flow_imbalance")
                    if fi is not None:
                        flow_lines.append(f"   Flow Imbalance: {format_scientific(fi, 4)}")
                    
                    # Ratio (apenas se vÃ¡lido)
                    if bsr is not None:
                        flow_lines.append(f"   Buy/Sell Ratio: {format_scientific(bsr, 2)}")
                    
                    if flow_lines:
                        order_flow_str = "\nğŸš° Fluxo de Ordens\n" + "\n".join(flow_lines) + "\n"
                
                except Exception as e:
                    logging.error(f"Erro ao processar order_flow: {e}")

        # ML FEATURES
        ml = event_data.get("ml_features") or event_data.get("ml") or {}
        ml_str = ""
        if isinstance(ml, dict) and ml:
            try:
                mf = ml.get("microstructure", {}) or {}
                
                tick_rule = mf.get("tick_rule_sum")
                flow_imb = mf.get("flow_imbalance")
                
                ml_lines = []
                if tick_rule is not None:
                    ml_lines.append(f"   Tick Rule Sum: {format_scientific(tick_rule, 3)}")
                if flow_imb is not None:
                    ml_lines.append(f"   Flow Imbalance: {format_scientific(flow_imb, 4)}")
                
                if ml_lines:
                    ml_str = "\nğŸ“ ML Features\n" + "\n".join(ml_lines) + "\n"
            except Exception:
                pass

        # OrderBook especÃ­fico
        if tipo_evento == "OrderBook" or "imbalance" in event_data:
            if not is_orderbook_valid:
                ob_str = f"""
ğŸ“Š Evento OrderBook - âš ï¸ DADOS INDISPONÃVEIS

ğŸ”´ ATENÃ‡ÃƒO: Orderbook zerado ou invÃ¡lido
   Bid Depth: ${bid_usd_raw:,.2f}
   Ask Depth: ${ask_usd_raw:,.2f}

âš ï¸ AnÃ¡lise de livro INDISPONÃVEL
   Use APENAS mÃ©tricas de fluxo se disponÃ­veis:
   - net_flow (delta acumulado)
   - flow_imbalance (proporÃ§Ã£o buy/sell)
   - tick_rule_sum (upticks vs downticks)
"""
            else:
                imbalance = ob_data.get("imbalance", 0)
                mid = ob_data.get("mid", 0)
                spread_pct = ob_data.get("spread_percent", 0)
                
                ob_str = f"""
ğŸ“Š Evento OrderBook âœ…

   PreÃ§o Mid: {format_price(mid)}
   Spread: {format_percent(spread_pct)}
   
   Profundidade (USD):
   â€¢ Bids: {format_large_number(bid_usd_raw)}
   â€¢ Asks: {format_large_number(ask_usd_raw)}
   
   Imbalance: {format_scientific(imbalance, 4)}
"""
            context = {
                "ativo": ativo,
                "tipo_evento": tipo_evento,
                "descricao": descricao,
                "ob_str": ob_str,
                "ml_str": ml_str,
                "vp_str": vp_str,
                "order_flow_str": order_flow_str,
                "multi_tf_str": multi_tf_str,
                "memoria_str": memoria_str,
                "prob_long": prob_long,
                "prob_short": prob_short,
                "prob_neutral": prob_neutral,
                "is_orderbook_valid": is_orderbook_valid,
            }
            return self._render_template("orderbook", context)

        # Prompt padrÃ£o
        vol_line = (
            "IndisponÃ­vel" if volume_total is None 
            else f"{format_large_number(volume_total)}"
        )
        delta_line = f"{format_delta(delta)}" if delta is not None else "IndisponÃ­vel"
        preco_fmt = format_price(preco)

        context = {
            "ativo": ativo,
            "tipo_evento": tipo_evento,
            "descricao": descricao,
            "preco_fmt": preco_fmt,
            "delta_line": delta_line,
            "vol_line": vol_line,
            "ml_str": ml_str,
            "vp_str": vp_str,
            "order_flow_str": order_flow_str,
            "multi_tf_str": multi_tf_str,
            "memoria_str": memoria_str,
            "prob_long": prob_long,
            "prob_short": prob_short,
            "prob_neutral": prob_neutral,
        }
        return self._render_template("default", context)

    def _build_structured_prompt(self, payload: Dict[str, Any]) -> str:
        """
        ConstrÃ³i o prompt usando o payload padronizado (ai_payload_builder).
        Se algum campo estiver faltando, faz fallback para 'N/A' ou valores simples.
        """
        meta = payload.get("signal_metadata", {}) or {}
        price = payload.get("price_context", {}) or {}
        flow = payload.get("flow_context", {}) or {}
        ob = payload.get("orderbook_context", {}) or {}
        macro = payload.get("macro_context", {}) or {}
        ml = payload.get("ml_features", {}) or {}
        hist = payload.get("historical_stats", {}) or {}

        symbol = payload.get("symbol") or meta.get("symbol") or "N/A"
        timestamp = payload.get("timestamp") or meta.get("timestamp") or "N/A"

        # PreÃ§o atual e OHLC
        ohlc = price.get("ohlc", {}) or {}
        current_price = format_price(price.get("current_price"))
        open_p = format_price(ohlc.get("open"))
        high_p = format_price(ohlc.get("high"))
        low_p = format_price(ohlc.get("low"))
        close_p = format_price(ohlc.get("close"))

        vp = price.get("volume_profile_daily", {}) or {}
        poc = format_price(vp.get("poc"))
        vah = format_price(vp.get("vah"))
        val = format_price(vp.get("val"))

        lines = []

        # CabeÃ§alho
        lines.append(f"AnÃ¡lise Institucional â€“ {symbol}")
        lines.append(f"{timestamp} | Tipo: {meta.get('type', 'N/A')}")
        lines.append(f"DescriÃ§Ã£o: {meta.get('description', 'Sem descriÃ§Ã£o')}")
        lines.append(f"Resultado da Batalha: {meta.get('battle_result', 'N/A')}")
        lines.append("")

        # PreÃ§o
        lines.append("CONTEXTO DE PREÃ‡O")
        lines.append(f"  â€¢ PreÃ§o Atual: {current_price}")
        lines.append(f"  â€¢ OHLC: O:{open_p} H:{high_p} L:{low_p} C:{close_p}")
        lines.append(f"  â€¢ VP DiÃ¡rio: POC {poc} | VAH {vah} | VAL {val}")
        lines.append("")

        # Fluxo bÃ¡sico (adapte conforme chaves reais do seu builder)
        net_flow = flow.get("net_flow")
        cvd_acc = flow.get("cvd_accumulated")
        if net_flow is not None or cvd_acc is not None:
            lines.append("CONTEXTO DE FLUXO")
            if net_flow is not None:
                lines.append(f"  â€¢ Net Flow (janela): {format_delta(net_flow)}")
            if cvd_acc is not None:
                lines.append(f"  â€¢ CVD acumulado: {format_delta(cvd_acc)}")
            lines.append("")

        # Orderbook (simplificado, adapte conforme builder)
        bid_usd = ob.get("bid_depth_usd")
        ask_usd = ob.get("ask_depth_usd")
        imbalance = ob.get("imbalance")
        if bid_usd is not None or ask_usd is not None:
            lines.append("ORDERBOOK / LIQUIDEZ")
            lines.append(
                f"  â€¢ Bids: {format_large_number(bid_usd)} | "
                f"Asks: {format_large_number(ask_usd)}"
            )
            if imbalance is not None:
                lines.append(f"  â€¢ Imbalance: {format_delta(imbalance)}")
            lines.append("")

        # Macro (simplificado)
        if macro:
            lines.append("MACRO / REGIME")
            session = macro.get("session") or macro.get("session_name")
            if session:
                lines.append(f"  â€¢ SessÃ£o: {session}")
            trends = macro.get("multi_timeframe_trends") or {}
            if trends:
                lines.append("  â€¢ TendÃªncias multi-timeframe:")
                for tf, tr in trends.items():
                    val = tr.get("tendencia") if isinstance(tr, dict) else tr
                    lines.append(f"    - {tf}: {val}")
            lines.append("")

        # HistÃ³rico (simplificado)
        if hist:
            lp = hist.get("long_prob")
            sp = hist.get("short_prob")
            np_ = hist.get("neutral_prob")
            lines.append("ESTATÃSTICA HISTÃ“RICA")
            lines.append(
                f"  â€¢ Probabilidades: Long={lp} | Short={sp} | Neutro={np_}"
            )
            lines.append("")

        # ============================================================
        # ğŸ†• INTELIGÃŠNCIA QUANTITATIVA (quant_model + ml_str)
        # ============================================================
        quant = payload.get("quant_model", {}) or {}
        ml_str_raw = payload.get("ml_str", "") or ""

        if quant:
            prob_up = quant.get("model_probability_up")
            prob_down = quant.get("model_probability_down")
            sentiment_model = quant.get("model_sentiment", "Indefinido")
            action_bias = quant.get("action_bias", "aguardar")
            confidence_model = quant.get("confidence_score", 0.0)
            features_used = quant.get("features_used", 0)
            total_features = quant.get("total_features", 0)

            lines.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            lines.append("ğŸ§  INTELIGÃŠNCIA QUANTITATIVA (XGBoost) â€“ USO OBRIGATÃ“RIO")
            lines.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
            if prob_up is not None:
                lines.append(f"  ğŸ“ˆ Probabilidade de Alta: {prob_up * 100:.1f}%")
            if prob_down is not None:
                lines.append(f"  ğŸ“‰ Probabilidade de Baixa: {prob_down * 100:.1f}%")
            lines.append(f"  ğŸ¯ ViÃ©s MatemÃ¡tico: {sentiment_model}")
            lines.append(f"  ğŸ”’ Action Bias (viÃ©s sugerido): {action_bias.upper()}")
            lines.append(f"  ğŸ“Š ConfianÃ§a do Modelo: {confidence_model * 100:.1f}%")
            lines.append(f"  ğŸ” Features usadas: {features_used}/{total_features}")
            lines.append("")
            lines.append("âš ï¸ REGRA CRÃTICA:")
            lines.append("   Esta inteligÃªncia quantitativa Ã© sua BASE PRINCIPAL de decisÃ£o.")
            lines.append("   SÃ³ vÃ¡ CONTRA o action_bias se o fluxo ou orderbook mostrarem")
            lines.append("   evidÃªncia MUITO FORTE (ex: absorÃ§Ã£o massiva, whale dump) na")
            lines.append("   direÃ§Ã£o oposta. Caso contrÃ¡rio, SIGA o viÃ©s matemÃ¡tico.")
            lines.append("")
        elif ml_str_raw:
            # Fallback: usa ml_str formatado se quant_model nÃ£o existir
            lines.append(ml_str_raw.strip())
            lines.append("")

        # InstruÃ§Ãµes
        lines.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        lines.append("ğŸ“‹ TAREFA DA IA")
        lines.append("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        lines.append("")
        lines.append("1) USE A INTELIGÃŠNCIA QUANTITATIVA COMO BASE PRINCIPAL.")
        lines.append("   O viÃ©s matemÃ¡tico (action_bias) deve guiar sua decisÃ£o inicial.")
        lines.append("")
        lines.append("2) CONFIRME OU INVALIDE com dados de fluxo e orderbook:")
        lines.append("   - CVD/Net Flow alinhados com o viÃ©s? â†’ CONFIRMA")
        lines.append("   - Orderbook com absorÃ§Ã£o forte? â†’ Pode indicar defesa/reversÃ£o")
        lines.append("   - Whale activity oposta ao viÃ©s? â†’ Sinal de alerta")
        lines.append("")
        lines.append("3) SÃ“ CONTRARIE O VIÃ‰S QUANTITATIVO se houver evidÃªncia MUITO FORTE:")
        lines.append("   - AbsorÃ§Ã£o massiva contrÃ¡ria")
        lines.append("   - Whale dump/pump significativo")
        lines.append("   - DivergÃªncia grave fluxo vs preÃ§o")
        lines.append("")
        lines.append("4) Defina regiÃ£o de entrada e zona de invalidaÃ§Ã£o (se houver setup).")
        lines.append("")
        lines.append("5) Se dados forem conflitantes ou confianÃ§a quantitativa baixa (<50%),")
        lines.append("   recomende aguardar com action='wait' ou 'avoid'.")

        return "\n".join(lines)

    # ====================================================================
    # CALLERS (SYNC + ASYNC)
    # ====================================================================

    async def _a_call_openai_text(self, prompt: str) -> str:
        """VersÃ£o assÃ­ncrona simples (texto livre) para OpenAI/Groq com fallbacks."""
        if not self.client_async:
            raise RuntimeError("Cliente assÃ­ncrono nÃ£o inicializado")

        # Usar lista centralizada de candidatos
        models_to_try = self._groq_model_candidates if self.mode == "groq" else [self.model_name]

        for model in models_to_try:
            try:
                response = await self.client_async.chat.completions.create(
                    model=model,
                    messages=[
                        {
                            "role": "system",
                            "content": SYSTEM_PROMPT,
                        },
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=700,
                    temperature=0.25,
                    timeout=30,
                )
                if response.choices and len(response.choices) > 0:
                    content = response.choices[0].message.content.strip()
                    # Sucesso: atualizar modelos se foi fallback
                    if model != self.model_name:
                        logging.info(f"ğŸ”„ Modelo trocado de {self.model_name} para {model} devido a decommissioned")
                        self.model_name = model
                        self.groq_model = model
                    return content
            except Exception as e:
                if _is_model_decommissioned_error(e):
                    logging.warning(f"Modelo {model} decommissioned. Tentando prÃ³ximo...")
                    continue
                else:
                    logging.error(f"Erro com modelo {model}: {e}. Tentando prÃ³ximo...")
                    continue

        # Todos falharam
        logging.error(f"Todos os modelos falharam para texto.")
        return ""



    def _call_openai_compatible(self, prompt: str, max_retries: int = 3) -> str:
        """
        Chama cliente OpenAI-compatÃ­vel de forma sÃ­ncrona (texto livre).
        Mantido para fallback e para ping.
        """
        base_delay = 1.0
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": SYSTEM_PROMPT,
                        },
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=700,
                    temperature=0.25,
                    timeout=30,
                )
                if response.choices and len(response.choices) > 0:
                    content = response.choices[0].message.content.strip()
                    if len(content) > 10:
                        if self.mode == "groq":
                            logging.debug(f"âœ… Groq respondeu ({len(content)} chars)")
                        return content
                return ""
            except Exception as e:
                logging.error(
                    f"Erro {self.mode.upper()} (tentativa {attempt+1}/{max_retries}): {e}"
                )
                if attempt < max_retries - 1:
                    time.sleep(base_delay * (2 ** attempt))
        return ""

    def _call_dashscope(self, prompt: str, max_retries: int = 3) -> str:
        """Chama DashScope API com retry (texto livre)."""
        base_delay = 1.0
        for attempt in range(max_retries):
            try:
                response = Generation.call(
                    model=self.model_name,
                    messages=[
                        {
                            "role": "system",
                            "content": SYSTEM_PROMPT,
                        },
                        {"role": "user", "content": prompt},
                    ],
                    result_format="message",
                    max_tokens=700,
                    temperature=0.25,
                    timeout=30,
                )
                content = _extract_dashscope_text(response).strip()
                if len(content) > 10:
                    return content
                return ""
            except Exception as e:
                logging.error(f"Erro DashScope (tentativa {attempt+1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(base_delay * (2 ** attempt))
        return ""

    def _call_model(self, prompt: str, event_data: Dict[str, Any]) -> tuple[str, Optional[Any]]:
        """
        Chama o provedor atual e retorna (raw_response, None).
        Sempre usa modo texto livre.
        """
        # Sempre usar texto livre (modo padrÃ£o)
        if self.mode in ("openai", "groq") and self.client:
            text = self._call_openai_compatible(prompt)
            return text, None
        elif self.mode == "dashscope":
            text = self._call_dashscope(prompt)
            return text, None
        else:
            text = self._generate_mock_analysis(event_data)
            return text, None

    def _generate_mock_analysis(self, event_data: Dict[str, Any]) -> str:
        """Gera anÃ¡lise mock quando IA indisponÃ­vel."""
        timestamp = self.time_manager.now_iso()
        mock_price = format_price(event_data.get('preco_fechamento', 0))
        mock_delta = format_delta(event_data.get('delta', 0))
        
        return (
            f"**InterpretaÃ§Ã£o (mock):** {event_data.get('tipo_evento')} em "
            f"{event_data.get('ativo')} Ã s {timestamp}.\n"
            f"PreÃ§o: ${mock_price} | Delta: {mock_delta}\n"
            f"**ForÃ§a:** {event_data.get('resultado_da_batalha')}\n"
            f"**Expectativa:** Monitorar reaÃ§Ã£o (dados limitados - modo mock)."
        )

    # ====================================================================
    # NÃšCLEO DE ANÃLISE (compartilhado entre analyze_event e analyze)
    # ====================================================================

    def _analyze_internal(self, event_data: Dict[str, Any]) -> tuple[str, Optional[Any]]:
        """
        NÃºcleo de anÃ¡lise: constrÃ³i prompt, chama modelo e retorna (texto, structured).
        """
        if not self.enabled:
            try:
                self._initialize_api()
            except Exception:
                pass
        
        if not self.enabled:
            return self._generate_mock_analysis(event_data), None

        if self._should_test_connection():
            self.last_test_time = time.time()
            if not self._test_connection():
                if self.connection_failed_count >= self.max_failures_before_mock:
                    return self._generate_mock_analysis(event_data), None

        try:
            prompt = self._create_prompt(event_data)
        except Exception as e:
            logging.error(f"Erro ao criar prompt: {e}", exc_info=True)
            return self._generate_mock_analysis(event_data), None

        try:
            raw, structured = self._call_model(prompt, event_data)
        except Exception as e:
            logging.error(f"Erro na chamada de IA: {e}", exc_info=True)
            raw, structured = self._generate_mock_analysis(event_data), None

        if not raw:
            raw = self._generate_mock_analysis(event_data)

        # Refresca heartbeat no final de uma anÃ¡lise bem-sucedida (se monitor disponÃ­vel)
        try:
            if self.health_monitor is not None:
                self.health_monitor.heartbeat(self.module_name)
        except Exception:
            pass

        return raw, structured

    # ====================================================================
    # INTERFACE PÃšBLICA
    # ====================================================================
    
    def analyze_event(self, event_data: Dict[str, Any]) -> str:
        """
        Analisa evento e retorna anÃ¡lise da IA (string).
        Mantido para compatibilidade com cÃ³digo legado.
        """
        try:
            analysis_text, _ = self._analyze_internal(event_data)
            return analysis_text
        except Exception as e:
            logging.error(f"Erro em analyze_event(): {e}", exc_info=True)
            return self._generate_mock_analysis(event_data)

    def analyze(self, event_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analisa evento e retorna resultado estruturado.

        Retorno:
            {
              "raw_response": str,
              "structured": dict | None,  # se JSON Mode + Pydantic disponÃ­veis
              "tipo_evento": str,
              "ativo": str,
              "timestamp": str,
              "success": bool,
              "mode": str,
              "model": str,
              "error": str (opcional)
            }
        """
        try:
            analysis_text, structured = self._analyze_internal(event_data)
            
            tipo_evento = event_data.get("tipo_evento", "N/A")
            ativo = event_data.get("ativo") or event_data.get("symbol") or "N/A"
            
            logging.info(f"âœ… IA [{self.mode or 'mock'}] analisou: {tipo_evento} - {ativo}")
            
            try:
                self.slog.info(
                    "ai_analyze_ok",
                    mode=self.mode or "mock",
                    model=self.model_name,
                    tipo_evento=tipo_evento,
                    ativo=ativo,
                )
            except Exception:
                pass
            
            return {
                "raw_response": analysis_text,
                "structured": (
                    structured.model_dump() if (PYDANTIC_AVAILABLE and structured is not None) else None
                ),
                "tipo_evento": tipo_evento,
                "ativo": ativo,
                "timestamp": self.time_manager.now_iso(),
                "success": True,
                "mode": self.mode or "mock",
                "model": self.model_name,
            }
            
        except Exception as e:
            logging.error(f"âŒ Erro em analyze(): {e}", exc_info=True)
            try:
                self.slog.error(
                    "ai_analyze_error",
                    error=str(e),
                    mode=self.mode or "mock",
                    model=self.model_name,
                    tipo_evento=event_data.get("tipo_evento", "N/A"),
                    ativo=event_data.get("ativo") or event_data.get("symbol") or "N/A",
                )
            except Exception:
                pass
            return {
                "raw_response": f"âŒ Erro ao analisar evento: {str(e)}",
                "structured": None,
                "tipo_evento": event_data.get("tipo_evento", "N/A"),
                "ativo": event_data.get("ativo") or event_data.get("symbol") or "N/A",
                "timestamp": self.time_manager.now_iso(),
                "success": False,
                "error": str(e),
                "mode": self.mode or "mock",
                "model": self.model_name,
            }

    def close(self):
        """Fecha conexÃ£o com IA e encerra heartbeat, se houver."""
        # Para heartbeat
        try:
            self._hb_stop.set()
            if self._hb_thread is not None and self._hb_thread.is_alive():
                self._hb_thread.join(timeout=5)
        except Exception:
            pass

        if self.mode == "groq":
            logging.info("ğŸ”Œ Desconectando GroqCloud...")
        self.client = None
        self.client_async = None

    def __del__(self):
        try:
            self.close()
        except Exception:
            pass


# ====================================================================
    # ğŸ§ª TESTE DE VALIDAÃ‡ÃƒO (executar diretamente)
# ====================================================================

if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("ğŸ§ª TESTANDO AI_ANALYZER v2.4.0 + PATCH 2 (GroqCloud - modo texto livre)")
    print("=" * 70)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)s | %(message)s'
    )
    
    analyzer = AIAnalyzer()
    
    print(f"\nâœ… Modo ativo: {analyzer.mode or 'MOCK'}")
    print(f"âœ… Modelo: {analyzer.model_name}")
    print(f"âœ… Enabled: {analyzer.enabled}")
    
    if analyzer.mode:
        print("\nğŸ” Testando conexÃ£o...")
        if analyzer._test_connection():
            print("âœ… ConexÃ£o OK!")
        else:
            print("âŒ Falha na conexÃ£o")
    
    print("\nğŸ“ Testando anÃ¡lise...")
    mock_event = {
        "tipo_evento": "AbsorÃ§Ã£o",
        "ativo": "BTCUSDT",
        "delta": -15.5,
        "volume_total": 125.3,
        "preco_fechamento": 95000,
        "resultado_da_batalha": "Vendedores",
    }
    
    result = analyzer.analyze(mock_event)
    
    print(f"\nğŸ“Š Resultado:")
    print(f"  Success: {result['success']}")
    print(f"  Modo: {result.get('mode', 'N/A')}")
    print(f"  Modelo: {result.get('model', 'N/A')}")
    print(f"  Structured: {result.get('structured')}")
    print(f"  Resposta ({len(result['raw_response'])} chars):")
    print(f"  {result['raw_response'][:300]}...")
    
    print("\n" + "=" * 70)
    print("âœ… TESTE CONCLUÃDO")
    print("=" * 70 + "\n")
